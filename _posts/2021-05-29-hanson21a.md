---
title: Learning Recurrent Neural Net Models of Nonlinear Systems
abstract: 'We consider the following learning problem: Given sample pairs of input
  and output signals generated by an unknown nonlinear system (which is not assumed
  to be causal or time-invariant), we wish to find a continuous-time recurrent neural
  net with hyperbolic tangent activation function that approximately reproduces the
  underlying i/o behavior with high confidence. Leveraging earlier work concerned
  with matching output derivatives up to a given finite order, we reformulate the
  learning problem in familiar system-theoretic language and derive quantitative guarantees
  on the sup-norm risk of the learned model in terms of the number of neurons, the
  sample size, the number of derivatives being matched, and the regularity properties
  of the inputs, the outputs, and the unknown i/o map.
  
  Erratum: There is an error in Theorem 4, which has propagated to the main result of the paper (Theorem 5). The corrected version has been posted to arXiv: https://arxiv.org/abs/2011.09573.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hanson21a
month: 0
tex_title: Learning Recurrent Neural Net Models of Nonlinear Systems
firstpage: 425
lastpage: 435
page: 425-435
order: 425
cycles: false
bibtex_author: Hanson, Joshua and Raginsky, Maxim and Sontag, Eduardo
author:
- given: Joshua
  family: Hanson
- given: Maxim
  family: Raginsky
- given: Eduardo
  family: Sontag
date: 2021-05-29
address:
container-title: Proceedings of the 3rd Conference on Learning for Dynamics and Control
volume: '144'
genre: inproceedings
issued:
  date-parts:
  - 2021
  - 5
  - 29
pdf: http://proceedings.mlr.press/v144/hanson21a/hanson21a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
